{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Alzheimer's Disease Prediction Model\n",
    "\n",
    "A deep learning model for classifying Alzheimer's disease severity from MRI images.\n",
    "\n",
    "**Classes:**\n",
    "- NonDemented\n",
    "- VeryMildDemented\n",
    "- MildDemented\n",
    "- ModerateDemented\n",
    "\n",
    "**Models:** CNN and ResNet architectures\n",
    "\n",
    "**Features:**\n",
    "- GPU optimization\n",
    "- Data augmentation\n",
    "- Hyperparameter tuning\n",
    "- Comprehensive evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install required packages and setup the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install scikit-learn matplotlib seaborn\n",
    "!pip install opencv-python-headless\n",
    "!pip install nibabel\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_data"
   },
   "source": [
    "## 2. Data Upload\n",
    "\n",
    "Upload your Alzheimer's dataset to Google Colab. The dataset should have the following structure:\n",
    "```\n",
    "Alzheimer_s Dataset/\n",
    "├── train/\n",
    "│   ├── MildDemented/\n",
    "│   ├── ModerateDemented/\n",
    "│   ├── NonDemented/\n",
    "│   └── VeryMildDemented/\n",
    "└── test/\n",
    "    ├── MildDemented/\n",
    "    ├── ModerateDemented/\n",
    "    ├── NonDemented/\n",
    "    └── VeryMildDemented/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_dataset"
   },
   "outputs": [],
   "source": [
    "# Option 1: Upload via Google Colab files\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Uncomment the following lines to upload your dataset as a zip file\n",
    "# uploaded = files.upload()\n",
    "# for filename in uploaded.keys():\n",
    "#     with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('/')\n",
    "\n",
    "# Option 2: Mount Google Drive (recommended)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set the path to your dataset (adjust as needed)\n",
    "# If you uploaded via option 1:\n",
    "# DATA_DIR = '/content/Alzheimer_s Dataset'\n",
    "\n",
    "# If you're using Google Drive:\n",
    "DATA_DIR = '/content/drive/MyDrive/Alzheimer_s Dataset'\n",
    "\n",
    "print(f'Data directory: {DATA_DIR}')\n",
    "print(f'Directory exists: {os.path.exists(DATA_DIR)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_preprocessing"
   },
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Custom dataset class and data preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_class"
   },
   "outputs": [],
   "source": [
    "class AlzheimerDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, class_names=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.class_names = class_names or ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f'Error loading image {image_path}: {e}')\n",
    "            # Return a black image in case of error\n",
    "            if self.transform:\n",
    "                image = self.transform(Image.new('RGB', (224, 224), (0, 0, 0)))\n",
    "            else:\n",
    "                image = torch.zeros(3, 224, 224)\n",
    "            return image, label\n",
    "\n",
    "def get_transforms(augment=True):\n",
    "    \"\"\"Get image transforms for training and validation\"\"\"\n",
    "    if augment:\n",
    "        # Training transforms with augmentation\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    # Validation/test transforms (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocessing_functions"
   },
   "outputs": [],
   "source": [
    "def load_data_from_folders(data_dir, class_names=None):\n",
    "    \"\"\"Load image paths and labels from folder structure\"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']\n",
    "    \n",
    "    class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    \n",
    "    all_image_paths = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = os.path.join(data_dir, split)\n",
    "        if not os.path.exists(split_dir):\n",
    "            continue\n",
    "            \n",
    "        for class_name in class_names:\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                continue\n",
    "                \n",
    "            # Get all image files\n",
    "            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "            image_files = []\n",
    "            for ext in image_extensions:\n",
    "                image_files.extend([f for f in os.listdir(class_dir) if f.lower().endswith(ext)])\n",
    "            \n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(class_dir, image_file)\n",
    "                all_image_paths.append(image_path)\n",
    "                all_labels.append(class_to_idx[class_name])\n",
    "    \n",
    "    return all_image_paths, all_labels, class_names, class_to_idx\n",
    "\n",
    "def split_train_val_test(data_dir, val_split=0.2, batch_size=32, augment=True, num_workers=2):\n",
    "    \"\"\"Create train, validation, and test dataloaders\"\"\"\n",
    "    class_names = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']\n",
    "    class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    \n",
    "    # Load training data\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    train_paths, train_labels = [], []\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(train_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "            image_files = []\n",
    "            for ext in image_extensions:\n",
    "                image_files.extend([f for f in os.listdir(class_dir) if f.lower().endswith(ext)])\n",
    "            \n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(class_dir, image_file)\n",
    "                train_paths.append(image_path)\n",
    "                train_labels.append(class_to_idx[class_name])\n",
    "    \n",
    "    # Load test data\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "    test_paths, test_labels = [], []\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(test_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "            image_files = []\n",
    "            for ext in image_extensions:\n",
    "                image_files.extend([f for f in os.listdir(class_dir) if f.lower().endswith(ext)])\n",
    "            \n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(class_dir, image_file)\n",
    "                test_paths.append(image_path)\n",
    "                test_labels.append(class_to_idx[class_name])\n",
    "    \n",
    "    # Split training data into train and validation\n",
    "    train_size = int((1 - val_split) * len(train_paths))\n",
    "    val_size = len(train_paths) - train_size\n",
    "    \n",
    "    # Create indices for splitting\n",
    "    indices = list(range(len(train_paths)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    # Split the data\n",
    "    train_paths_split = [train_paths[i] for i in train_indices]\n",
    "    train_labels_split = [train_labels[i] for i in train_indices]\n",
    "    val_paths = [train_paths[i] for i in val_indices]\n",
    "    val_labels = [train_labels[i] for i in val_indices]\n",
    "    \n",
    "    # Get transforms\n",
    "    train_transform, val_transform = get_transforms(augment)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AlzheimerDataset(train_paths_split, train_labels_split, train_transform, class_names)\n",
    "    val_dataset = AlzheimerDataset(val_paths, val_labels, val_transform, class_names)\n",
    "    test_dataset = AlzheimerDataset(test_paths, test_labels, val_transform, class_names)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f'Dataset Statistics:')\n",
    "    print(f'Training samples: {len(train_dataset)}')\n",
    "    print(f'Validation samples: {len(val_dataset)}')\n",
    "    print(f'Test samples: {len(test_dataset)}')\n",
    "    print(f'Number of classes: {len(class_names)}')\n",
    "    print(f'Class names: {class_names}')\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_names, class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_architecture"
   },
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "Define CNN and ResNet models for Alzheimer's classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnn_model"
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=4, dropout_rate=0.5):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Pooling and dropout\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second conv block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third conv block\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "resnet_model"
   },
   "outputs": [],
   "source": [
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, num_classes=4, dropout_rate=0.5, pretrained=True):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet18\n",
    "        self.resnet = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Replace the final layer\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "def get_model(model_type='cnn', num_classes=4, dropout_rate=0.5):\n",
    "    \"\"\"Factory function to get model\"\"\"\n",
    "    if model_type.lower() == 'cnn':\n",
    "        return CNNModel(num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "    elif model_type.lower() == 'resnet':\n",
    "        return ResNetModel(num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model type: {model_type}. Choose from [\"cnn\", \"resnet\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_functions"
   },
   "source": [
    "## 5. Training Functions\n",
    "\n",
    "Functions for training and evaluating the model."
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_functions"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_predictions, all_targets\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                num_epochs, device, save_path=None):\n",
    "    \"\"\"Train the model for multiple epochs\"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            if save_path:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f'New best model saved with validation accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization_functions"
   },
   "source": [
    "## 6. Visualization Functions\n",
    "\n",
    "Functions for plotting training history and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_functions"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Plot training and validation loss and accuracy\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Training Loss', marker='o')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss', marker='s')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Training Accuracy', marker='o')\n",
    "    ax2.plot(history['val_acc'], label='Validation Accuracy', marker='s')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_batch(data_loader, class_names, num_samples=8):\n",
    "    \"\"\"Visualize a batch of images with their labels\"\"\"\n",
    "    # Get a batch of data\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # Select samples to display\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    \n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Denormalize image\n",
    "        img = images[i]\n",
    "        img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        # Convert to numpy and transpose\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].imshow(img_np)\n",
    "        axes[i].set_title(f'Class: {class_names[labels[i]]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyperparameter_tuning"
   },
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "Grid search for optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyperparam_tuning"
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(train_loader, val_loader, num_classes, device, num_epochs=10):\n",
    "    \"\"\"Perform hyperparameter tuning using grid search\"\"\"\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'model_type': ['cnn', 'resnet'],\n",
    "        'optimizer': ['adam', 'sgd'],\n",
    "        'learning_rate': [0.001, 0.0001],\n",
    "        'use_scheduler': [True, False]\n",
    "    }\n",
    "    \n",
    "    best_params = None\n",
    "    best_val_acc = 0.0\n",
    "    results = []\n",
    "    \n",
    "    # Grid search\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f'\\nTesting parameters: {params}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Create model\n",
    "        model = get_model(model_type=params['model_type'], num_classes=num_classes)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Define optimizer\n",
    "        if params['optimizer'] == 'adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        else:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=0.9)\n",
    "        \n",
    "        # Define scheduler\n",
    "        scheduler = None\n",
    "        if params['use_scheduler']:\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "        \n",
    "        # Train model\n",
    "        history = train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                              scheduler, num_epochs, device)\n",
    "        \n",
    "        # Get best validation accuracy\n",
    "        val_acc = max(history['val_acc'])\n",
    "        \n",
    "        # Store results\n",
    "        result = params.copy()\n",
    "        result['val_acc'] = val_acc\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f'Validation Accuracy: {val_acc:.2f}%')\n",
    "        \n",
    "        # Update best parameters\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params = params.copy()\n",
    "    \n",
    "    print(f'\\nBest parameters: {best_params}')\n",
    "    print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    \n",
    "    # Convert results to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f'\\nAll results:')\n",
    "    print(results_df.sort_values('val_acc', ascending=False))\n",
    "    \n",
    "    return best_params, best_val_acc, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "main_execution"
   },
   "source": [
    "## 8. Main Execution\n",
    "\n",
    "Load data, train model, and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "MODEL_TYPE = 'resnet'  # 'cnn' or 'resnet'\n",
    "USE_AUGMENTATION = True\n",
    "DROPOUT_RATE = 0.5\n",
    "VAL_SPLIT = 0.2\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '/content/results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f'Configuration:')\n",
    "print(f'Data directory: {DATA_DIR}')\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Number of epochs: {NUM_EPOCHS}')\n",
    "print(f'Learning rate: {LEARNING_RATE}')\n",
    "print(f'Model type: {MODEL_TYPE}')\n",
    "print(f'Use augmentation: {USE_AUGMENTATION}')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "print('Loading data...')\n",
    "train_loader, val_loader, test_loader, class_names, class_to_idx = split_train_val_test(\n",
    "    DATA_DIR, \n",
    "    val_split=VAL_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=USE_AUGMENTATION,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f'\\nClass mapping: {class_to_idx}')\n",
    "print(f'Number of classes: {len(class_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_data"
   },
   "outputs": [],
   "source": [
    "# Visualize some training data\n",
    "print('Visualizing training data samples...')\n",
    "visualize_batch(train_loader, class_names, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(f'Creating {MODEL_TYPE} model...')\n",
    "model = get_model(model_type=MODEL_TYPE, num_classes=len(class_names), dropout_rate=DROPOUT_RATE)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(f'Model created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optional_hyperparameter_tuning"
   },
   "source": [
    "## 9. Optional: Hyperparameter Tuning\n",
    "\n",
    "Uncomment the following cell to perform hyperparameter tuning (this will take longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_hyperparameter_tuning"
   },
   "outputs": [],
   "source": [
    "# Uncomment to run hyperparameter tuning\n",
    "# print('Starting hyperparameter tuning...')\n",
    "# best_params, best_val_acc, results_df = hyperparameter_tuning(\n",
    "#     train_loader, val_loader, len(class_names), device, num_epochs=10\n",
    "# )\n",
    "# \n",
    "# # Update model with best parameters\n",
    "# MODEL_TYPE = best_params['model_type']\n",
    "# LEARNING_RATE = best_params['learning_rate']\n",
    "# \n",
    "# # Recreate model with best parameters\n",
    "# model = get_model(model_type=MODEL_TYPE, num_classes=len(class_names), dropout_rate=DROPOUT_RATE)\n",
    "# model = model.to(device)\n",
    "# \n",
    "# if best_params['optimizer'] == 'adam':\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# else:\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "# \n",
    "# if best_params['use_scheduler']:\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "# else:\n",
    "#     scheduler = None\n",
    "# \n",
    "# print(f'Using best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print('Starting training...')\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    device=device,\n",
    "    save_path=os.path.join(output_dir, 'best_model.pth')\n",
    ")\n",
    "\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_history"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "print('Plotting training history...')\n",
    "plot_training_history(history, save_path=os.path.join(output_dir, 'training_history.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 10. Model Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print('Evaluating on test set...')\n",
    "test_loss, test_acc, test_predictions, test_targets = evaluate_model(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "\n",
    "# Calculate detailed metrics\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(\n",
    "    test_targets, test_predictions, average=None\n",
    ")\n",
    "\n",
    "# Print per-class metrics\n",
    "print('\\nPer-class metrics:')\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f'{class_name}:')\n",
    "    print(f'  Precision: {precision[i]:.4f}')\n",
    "    print(f'  Recall: {recall[i]:.4f}')\n",
    "    print(f'  F1-score: {f1_score[i]:.4f}')\n",
    "    print(f'  Support: {support[i]}')\n",
    "    print()\n",
    "\n",
    "# Overall metrics\n",
    "overall_precision = np.mean(precision)\n",
    "overall_recall = np.mean(recall)\n",
    "overall_f1 = np.mean(f1_score)\n",
    "\n",
    "print(f'Overall Metrics:')\n",
    "print(f'Precision: {overall_precision:.4f}')\n",
    "print(f'Recall: {overall_recall:.4f}')\n",
    "print(f'F1-score: {overall_f1:.4f}')\n",
    "\n",
    "# Classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(test_targets, test_predictions, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_confusion_matrix"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "print('Plotting confusion matrix...')\n",
    "plot_confusion_matrix(\n",
    "    test_targets, test_predictions, class_names, \n",
    "    save_path=os.path.join(output_dir, 'confusion_matrix.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## 11. Save Results\n",
    "\n",
    "Save model, metrics, and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_metrics"
   },
   "outputs": [],
   "source": [
    "# Save metrics and configuration\n",
    "results = {\n",
    "    'model_type': MODEL_TYPE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout_rate': DROPOUT_RATE,\n",
    "    'use_augmentation': USE_AUGMENTATION,\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_loss': test_loss,\n",
    "    'overall_precision': overall_precision,\n",
    "    'overall_recall': overall_recall,\n",
    "    'overall_f1': overall_f1,\n",
    "    'class_names': class_names,\n",
    "    'class_to_idx': class_to_idx,\n",
    "    'per_class_metrics': {\n",
    "        class_names[i]: {\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1_score': f1_score[i],\n",
    "            'support': int(support[i])\n",
    "        } for i in range(len(class_names))\n",
    "    },\n",
    "    'training_history': history,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save results as JSON\n",
    "with open(os.path.join(output_dir, 'results.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f'Results saved to {output_dir}')\n",
    "print(f'Files saved:')\n",
    "print(f'- best_model.pth: Trained model weights')\n",
    "print(f'- results.json: Detailed metrics and configuration')\n",
    "print(f'- training_history.png: Training curves')\n",
    "print(f'- confusion_matrix.png: Confusion matrix')\n",
    "\n",
    "# List all files in output directory\n",
    "print(f'\\nOutput directory contents:')\n",
    "for file in os.listdir(output_dir):\n",
    "    print(f'- {file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## 12. Inference Function\n",
    "\n",
    "Function to make predictions on new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_function"
   },
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, class_names, device, transform=None):\n",
    "    \"\"\"Make prediction on a single image\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    if transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Get prediction results\n",
    "    predicted_class = class_names[predicted.item()]\n",
    "    confidence = probabilities[0][predicted.item()].item()\n",
    "    \n",
    "    # Get all class probabilities\n",
    "    class_probabilities = {\n",
    "        class_names[i]: probabilities[0][i].item() \n",
    "        for i in range(len(class_names))\n",
    "    }\n",
    "    \n",
    "    return predicted_class, confidence, class_probabilities\n",
    "\n",
    "# Example usage (uncomment to test with a specific image)\n",
    "# image_path = '/path/to/your/test/image.jpg'\n",
    "# predicted_class, confidence, class_probs = predict_image(model, image_path, class_names, device)\n",
    "# print(f'Predicted class: {predicted_class}')\n",
    "# print(f'Confidence: {confidence:.4f}')\n",
    "# print(f'All class probabilities: {class_probs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_results"
   },
   "source": [
    "## 13. Download Results\n",
    "\n",
    "Download the trained model and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Create a zip file with all results\n",
    "import zipfile\n",
    "\n",
    "zip_path = '/content/alzheimer_model_results.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, output_dir)\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(f'Results packaged in: {zip_path}')\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download(zip_path)\n",
    "\n",
    "print('Download started! Check your browser downloads.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a complete pipeline for Alzheimer's disease classification using deep learning:\n",
    "\n",
    "1. **Data Loading**: Supports the standard train/test folder structure\n",
    "2. **Preprocessing**: Image augmentation and normalization\n",
    "3. **Models**: CNN and ResNet architectures\n",
    "4. **Training**: GPU-optimized training with validation\n",
    "5. **Evaluation**: Comprehensive metrics and visualizations\n",
    "6. **Hyperparameter Tuning**: Grid search for optimal parameters\n",
    "7. **Inference**: Easy prediction on new images\n",
    "\n",
    "**Key Features:**\n",
    "- 4-class classification (NonDemented, VeryMildDemented, MildDemented, ModerateDemented)\n",
    "- GPU acceleration with Google Colab\n",
    "- Data augmentation for better generalization\n",
    "- Comprehensive evaluation metrics\n",
    "- Model saving and downloading\n",
    "\n",
    "**To use this notebook:**\n",
    "1. Upload your dataset to Google Drive or directly to Colab\n",
    "2. Set the correct DATA_DIR path\n",
    "3. Run all cells sequentially\n",
    "4. Download the trained model and results\n",
    "\n",
    "The trained model can be used for inference on new MRI images to predict Alzheimer's disease severity."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

